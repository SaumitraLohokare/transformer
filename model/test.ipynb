{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test the decoder that we've built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from decoder import DecoderBlock\n",
    "from common import TransformerEmbeddings, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[813, 137, 291, 776, 847, 865, 852, 323, 508, 554]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(high=1000, size=(1, 10,)) # The extra dimension at the start is for batch size.\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len, n_heads, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = TransformerEmbeddings(vocab_size, embed_dim, max_seq_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_dim, max_seq_len, n_heads, hidden_size, False)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln = LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        embeddings = self.embeddings(ids)\n",
    "        for block in self.layers:\n",
    "            outputs = block(embeddings)\n",
    "        outputs = self.ln(outputs)\n",
    "        logits = self.lm_head(outputs)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(\n",
    "    vocab_size=20000, \n",
    "    embed_dim=1024, \n",
    "    max_seq_len=2048, \n",
    "    n_heads=16, \n",
    "    hidden_size=4096, \n",
    "    num_layers=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'412,309,024 Parameters'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{sum(p.numel() for p in gpt.parameters()):,} Parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 20000])\n",
      "================================================================================\n",
      "tensor([[[ 0.1072,  0.0526,  0.5062,  ...,  0.0578, -0.6875,  0.4822],\n",
      "         [ 1.4474,  0.4296,  0.2251,  ...,  0.5610, -0.3306,  0.3003],\n",
      "         [ 0.3551, -0.9697,  0.4853,  ...,  0.3207,  0.1015,  0.6030],\n",
      "         ...,\n",
      "         [ 0.3563, -0.7549,  0.8186,  ..., -0.0430, -0.9175, -0.3486],\n",
      "         [ 0.1267,  0.0051,  0.3541,  ...,  0.4223, -0.2188, -0.0300],\n",
      "         [-0.5174,  0.1711,  0.2495,  ..., -0.3418, -0.4729,  0.8262]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Predicted token: 16016\n"
     ]
    }
   ],
   "source": [
    "logits = gpt(input_ids)\n",
    "print(logits.size())\n",
    "print('=' * 80)\n",
    "print(logits)\n",
    "\n",
    "token = torch.argmax(logits[:,-1,:])\n",
    "print(f'Predicted token: {token.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
