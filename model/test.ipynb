{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test the decoder that we've built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from decoder import DecoderBlock, FlashDecoderBlock\n",
    "from common import TransformerEmbeddings, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[743, 252, 409, 858, 127, 606, 571, 435, 353, 275]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(high=1000, size=(1, 10,)) # The extra dimension at the start is for batch size.\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len, n_heads, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = TransformerEmbeddings(vocab_size, embed_dim, max_seq_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_dim, max_seq_len, n_heads, hidden_size, False)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln = LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        embeddings = self.embeddings(ids)\n",
    "        for block in self.layers:\n",
    "            outputs = block(embeddings)\n",
    "        outputs = self.ln(outputs)\n",
    "        logits = self.lm_head(outputs)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saumi/dev/projects/SmallCoder/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(\n",
    "    vocab_size=tokenizer.n_vocab, \n",
    "    embed_dim=768, \n",
    "    max_seq_len=1024, \n",
    "    n_heads=16, \n",
    "    hidden_size=768 * 2, \n",
    "    num_layers=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'237,550,517 Parameters'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{sum(p.numel() for p in gpt.parameters()):,} Parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 100277])\n",
      "================================================================================\n",
      "tensor([[[-0.2122,  0.5439,  0.2116,  ...,  0.4221,  0.7646, -0.8687],\n",
      "         [-0.0962,  0.3857, -0.6739,  ..., -0.2257,  0.3856, -0.2822],\n",
      "         [ 0.3044, -0.6492,  0.1518,  ...,  0.7394, -0.4433, -0.9983],\n",
      "         ...,\n",
      "         [-0.2844, -0.3886, -0.5908,  ..., -0.7100, -0.3606, -0.9481],\n",
      "         [-0.2139,  0.0526,  0.3626,  ..., -0.1844, -0.6128, -0.3698],\n",
      "         [-0.7062, -0.6437, -0.1544,  ..., -0.0406, -0.0345,  1.0520]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Predicted token: 76899\n"
     ]
    }
   ],
   "source": [
    "logits = gpt(input_ids)\n",
    "print(logits.size())\n",
    "print('=' * 80)\n",
    "print(logits)\n",
    "\n",
    "token = torch.argmax(logits[:,-1,:])\n",
    "print(f'Predicted token: {token.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Flash Attention GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len, n_heads, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = TransformerEmbeddings(vocab_size, embed_dim, max_seq_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            FlashDecoderBlock(embed_dim, max_seq_len, n_heads, hidden_size, False)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln = LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        embeddings = self.embeddings(ids)\n",
    "        for block in self.layers:\n",
    "            outputs = block(embeddings)\n",
    "        outputs = self.ln(outputs)\n",
    "        logits = self.lm_head(outputs)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flashGPT = FlashGPT(\n",
    "    vocab_size=tokenizer.n_vocab, \n",
    "    embed_dim=768, \n",
    "    max_seq_len=1024, \n",
    "    n_heads=16, \n",
    "    hidden_size=768 * 2, \n",
    "    num_layers=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 100277])\n",
      "================================================================================\n",
      "tensor([[[-0.9890, -0.4890,  0.1613,  ..., -0.6838, -0.6599,  1.1461],\n",
      "         [-0.9446,  0.6658, -0.2388,  ...,  0.1858,  0.2388,  0.6867],\n",
      "         [ 0.4036, -0.3223, -0.2047,  ...,  0.6309, -0.3606, -0.1010],\n",
      "         ...,\n",
      "         [-0.0617,  0.5768, -0.5326,  ..., -0.2657, -0.2882,  0.4066],\n",
      "         [-0.1702,  0.0284,  0.4018,  ...,  0.0069,  0.6360,  0.1791],\n",
      "         [-0.4051, -0.3799,  0.4261,  ..., -0.2515, -0.8847,  0.0564]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Predicted token: 62598\n"
     ]
    }
   ],
   "source": [
    "logits = flashGPT(input_ids)\n",
    "print(logits.size())\n",
    "print('=' * 80)\n",
    "print(logits)\n",
    "\n",
    "token = torch.argmax(logits[:,-1,:])\n",
    "print(f'Predicted token: {token.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
