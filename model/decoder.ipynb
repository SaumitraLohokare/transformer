{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\"  width=\"400\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most of the steps below are the same as encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 45, 583, 100, 714,  50, 763,  90,   5, 890, 965]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(high=1000, size=(1, 10,)) # The extra dimension at the start is for batch size.\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000 # This is how many tokens do we have in our vocabulary\n",
    "embed_dim = 4 # This is how many dimensions do we have\n",
    "seq_len = 10\n",
    "batch_size = 1\n",
    "token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "position_embedding = nn.Embedding(seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1858, -0.8513, -0.8483,  0.3233],\n",
      "        [-1.1369,  1.3385, -1.3545, -0.1748],\n",
      "        [-1.1121, -2.0867, -0.3455,  0.6971],\n",
      "        [ 1.5121, -0.5488,  0.3287,  0.1512],\n",
      "        [ 1.3869,  0.2724,  0.3173,  1.2025],\n",
      "        [ 0.4041, -1.6461,  0.6674, -0.1767],\n",
      "        [-1.5414, -1.3549,  0.6292, -1.5743],\n",
      "        [-0.6163,  0.6783,  0.4672,  0.9271],\n",
      "        [ 1.2818, -0.4587,  1.4878,  0.3366],\n",
      "        [-1.3596,  1.6736,  0.9903, -0.1597]], grad_fn=<EmbeddingBackward0>)\n",
      "====================\n",
      "torch.Size([10, 4])\n"
     ]
    }
   ],
   "source": [
    "positions = torch.arange(0, seq_len, dtype=torch.long)\n",
    "pos_embed = position_embedding(positions)\n",
    "print(pos_embed)\n",
    "print('='*20)\n",
    "print(pos_embed.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7674, -2.2268, -0.6793, -0.8814],\n",
      "         [-0.5230, -1.3035,  0.6975,  0.9871],\n",
      "         [ 1.8228, -0.5018,  0.6378,  0.9297],\n",
      "         [-0.1088, -0.2072, -1.6853,  0.2031],\n",
      "         [ 1.9600,  0.2219, -0.8571,  1.3081],\n",
      "         [-0.1358, -0.7490, -1.2744, -0.4434],\n",
      "         [ 0.7796,  1.1134, -0.2814, -1.5582],\n",
      "         [-0.2071, -0.0496,  0.8892, -0.0980],\n",
      "         [-0.9709,  0.3850, -0.6061,  0.3085],\n",
      "         [-0.5543,  0.1082,  1.4128,  0.7548]]], grad_fn=<EmbeddingBackward0>)\n",
      "====================\n",
      "torch.Size([1, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "input_embed = token_embedding(input_ids)\n",
    "print(input_embed)\n",
    "print('='*20)\n",
    "print(input_embed.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5816, -3.0781, -1.5276, -0.5581],\n",
      "         [-1.6599,  0.0350, -0.6570,  0.8123],\n",
      "         [ 0.7107, -2.5884,  0.2923,  1.6268],\n",
      "         [ 1.4034, -0.7560, -1.3567,  0.3543],\n",
      "         [ 3.3469,  0.4942, -0.5398,  2.5106],\n",
      "         [ 0.2684, -2.3951, -0.6070, -0.6201],\n",
      "         [-0.7619, -0.2415,  0.3479, -3.1324],\n",
      "         [-0.8235,  0.6287,  1.3564,  0.8291],\n",
      "         [ 0.3110, -0.0737,  0.8817,  0.6451],\n",
      "         [-1.9139,  1.7818,  2.4031,  0.5951]]], grad_fn=<AddBackward0>)\n",
      "====================\n",
      "torch.Size([1, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "input_embed = input_embed + pos_embed\n",
    "print(input_embed)\n",
    "print('='*20)\n",
    "print(input_embed.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Self Attention ‼️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be used in the addition step after we perform attention\n",
    "residual_embeddings_1 = input_embed.clone() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For normalization we actually have to use trainable parameters\n",
    "epsilon = nn.Parameter(torch.ones(embed_dim))\n",
    "gamma = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "input_embed = F.layer_norm(input_embed, epsilon.shape, epsilon, gamma, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 4]) torch.Size([1, 10, 4]) torch.Size([1, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "# This Linear layer will help us split our embeddings into Query, Key, and Value\n",
    "qkv_layer = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "\n",
    "q, k, v = qkv_layer(input_embed).split(embed_dim, dim=2)\n",
    "print(q.size(), k.size(), v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4452, -0.5987, -0.0388, -0.1329],\n",
      "         [ 0.3837, -0.2841,  0.6227, -0.9738],\n",
      "         [ 0.4625, -0.4140,  0.1296, -0.2104],\n",
      "         [ 0.3667, -1.1854, -0.4079, -0.1858],\n",
      "         [ 0.4568, -1.2830, -0.3102, -0.3649],\n",
      "         [ 0.2107, -0.2645, -0.2229,  0.2975],\n",
      "         [-0.6930,  1.1213, -0.2938,  0.9906],\n",
      "         [-0.0481,  0.8567,  0.6346, -0.2328],\n",
      "         [ 0.1515,  0.5416,  0.3539,  0.0995],\n",
      "         [-0.2398,  1.0069,  0.5121, -0.0524]]], grad_fn=<SplitBackward0>)\n",
      "tensor([[[[ 0.4452, -0.5987],\n",
      "          [ 0.3837, -0.2841],\n",
      "          [ 0.4625, -0.4140],\n",
      "          [ 0.3667, -1.1854],\n",
      "          [ 0.4568, -1.2830],\n",
      "          [ 0.2107, -0.2645],\n",
      "          [-0.6930,  1.1213],\n",
      "          [-0.0481,  0.8567],\n",
      "          [ 0.1515,  0.5416],\n",
      "          [-0.2398,  1.0069]],\n",
      "\n",
      "         [[-0.0388, -0.1329],\n",
      "          [ 0.6227, -0.9738],\n",
      "          [ 0.1296, -0.2104],\n",
      "          [-0.4079, -0.1858],\n",
      "          [-0.3102, -0.3649],\n",
      "          [-0.2229,  0.2975],\n",
      "          [-0.2938,  0.9906],\n",
      "          [ 0.6346, -0.2328],\n",
      "          [ 0.3539,  0.0995],\n",
      "          [ 0.5121, -0.0524]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's say we have 3 heads in out multi head attention\n",
    "n_head = 2 # IMP: Embedding dimension should be divisible by number of heads\n",
    "\n",
    "print(q)\n",
    "\n",
    "k = k.view(batch_size, seq_len, n_head, embed_dim // n_head).transpose(1, 2) # (Batch size, num_heads, sequence length, embeddings per head)\n",
    "q = q.view(batch_size, seq_len, n_head, embed_dim // n_head).transpose(1, 2) # (Batch size, num_heads, sequence length, embeddings per head)\n",
    "v = v.view(batch_size, seq_len, n_head, embed_dim // n_head).transpose(1, 2) # (Batch size, num_heads, sequence length, embeddings per head)\n",
    "\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the causal attention mask first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "raw_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 10, 10])\n",
      "tensor([[[[-0.0278,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [ 0.0692, -0.2711,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [ 0.0547, -0.3689,  0.0170,    -inf,    -inf,    -inf,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [-0.3012, -0.7969, -0.4466,  0.0391,    -inf,    -inf,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [-0.2974, -0.8787, -0.4524,  0.0446, -0.1173,    -inf,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [-0.0056, -0.2127, -0.0330,  0.0136, -0.0235,  0.0786,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [ 0.1195,  0.8475,  0.2435, -0.0500,  0.1007, -0.2208, -0.7444,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [ 0.3214,  0.5171,  0.4349, -0.0201,  0.0799,  0.1012, -0.5470,\n",
      "            0.3458,    -inf,    -inf],\n",
      "          [ 0.2901,  0.2776,  0.3690, -0.0059,  0.0516,  0.1660, -0.3376,\n",
      "            0.2281,  0.3662,    -inf],\n",
      "          [ 0.2901,  0.6574,  0.4164, -0.0305,  0.0928,  0.0162, -0.6512,\n",
      "            0.3968,  0.3664,  0.1817]],\n",
      "\n",
      "         [[-0.0426,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [-0.0197, -0.0305,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [-0.0059, -0.0068, -0.0282,    -inf,    -inf,    -inf,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [-0.1733, -0.0266, -0.1425, -0.2350,    -inf,    -inf,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [-0.1824, -0.0329, -0.1693, -0.1758, -0.1916,    -inf,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [-0.0045,  0.0080,  0.0310, -0.1337, -0.1357,  0.0142,    -inf,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [ 0.1294,  0.0444,  0.2040, -0.1840, -0.1767,  0.1358,  0.0199,\n",
      "              -inf,    -inf,    -inf],\n",
      "          [ 0.1517,  0.0119,  0.0794,  0.3725,  0.3885,  0.0796, -0.2729,\n",
      "           -0.3132,    -inf,    -inf],\n",
      "          [ 0.1364,  0.0196,  0.1068,  0.2047,  0.2175,  0.0892, -0.1798,\n",
      "           -0.1457, -0.0482,    -inf],\n",
      "          [ 0.1530,  0.0173,  0.1010,  0.2988,  0.3140,  0.0907, -0.2365,\n",
      "           -0.2356, -0.1299, -0.2660]]]], grad_fn=<MaskedFillBackward0>)\n",
      "====================\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.5843, 0.4157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3820, 0.2501, 0.3679, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2578, 0.1570, 0.2229, 0.3623, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1992, 0.1114, 0.1706, 0.2804, 0.2385, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1702, 0.1384, 0.1656, 0.1735, 0.1672, 0.1851, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1396, 0.2892, 0.1581, 0.1179, 0.1370, 0.0994, 0.0589, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1413, 0.1718, 0.1582, 0.1004, 0.1109, 0.1133, 0.0593, 0.1447,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1244, 0.1229, 0.1346, 0.0925, 0.0980, 0.1099, 0.0664, 0.1169,\n",
      "           0.1343, 0.0000],\n",
      "          [0.1068, 0.1542, 0.1212, 0.0775, 0.0877, 0.0812, 0.0417, 0.1188,\n",
      "           0.1153, 0.0958]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.5027, 0.4973, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.3359, 0.3356, 0.3285, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.2422, 0.2804, 0.2497, 0.2277, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1934, 0.2245, 0.1959, 0.1946, 0.1916, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1717, 0.1739, 0.1779, 0.1509, 0.1506, 0.1750, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1571, 0.1443, 0.1693, 0.1148, 0.1157, 0.1581, 0.1408, 0.0000,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1329, 0.1155, 0.1236, 0.1657, 0.1684, 0.1236, 0.0869, 0.0835,\n",
      "           0.0000, 0.0000],\n",
      "          [0.1207, 0.1074, 0.1172, 0.1292, 0.1309, 0.1151, 0.0880, 0.0910,\n",
      "           0.1004, 0.0000],\n",
      "          [0.1129, 0.0985, 0.1071, 0.1306, 0.1326, 0.1060, 0.0765, 0.0765,\n",
      "           0.0851, 0.0742]]]], grad_fn=<SoftmaxBackward0>)\n",
      "====================\n",
      "tensor([[[ 0.6710, -0.5690,  0.6621,  0.2080],\n",
      "         [ 0.4994, -0.6390,  0.2085,  0.0858],\n",
      "         [ 0.5440, -0.7012,  0.3121,  0.1590],\n",
      "         [ 0.6260, -0.3446,  0.4076,  0.0553],\n",
      "         [ 0.6833, -0.2255,  0.4893, -0.0138],\n",
      "         [ 0.6017, -0.3175,  0.5146,  0.0466],\n",
      "         [ 0.4597, -0.3594,  0.3731,  0.0730],\n",
      "         [ 0.3411, -0.4000,  0.3744,  0.0403],\n",
      "         [ 0.2917, -0.4446,  0.2931,  0.1145],\n",
      "         [ 0.2084, -0.4842,  0.2385,  0.1099]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "attention = (q @ k.transpose(-2, -1)) / math.sqrt(embed_dim // n_head) # In multi-head attention, Dk becomes the dimension of embeddings per head\n",
    "\n",
    "print(attention.size())\n",
    "\n",
    "# Here we should insert our Alibi mask\n",
    "attention = attention.masked_fill(raw_mask[:,:,:seq_len,:seq_len] == 0, float('-inf'))\n",
    "#                                                ^ This is not max_seq_len but rather the length of current sequence\n",
    "\n",
    "print(attention)\n",
    "print('=' * 20)\n",
    "\n",
    "attention = F.softmax(attention, dim=-1)\n",
    "\n",
    "print(attention)\n",
    "print('=' * 20)\n",
    "\n",
    "new_embeddings = attention @ v\n",
    "new_embeddings = new_embeddings.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim) # re-assemble all head outputs side by side\n",
    "\n",
    "print(new_embeddings)\n",
    "print(new_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0895, -3.6472, -0.8655, -0.3501],\n",
      "         [-1.1605, -0.6040, -0.4485,  0.8981],\n",
      "         [ 1.2547, -3.2896,  0.6044,  1.7858],\n",
      "         [ 2.0294, -1.1006, -0.9491,  0.4096],\n",
      "         [ 4.0302,  0.2687, -0.0506,  2.4968],\n",
      "         [ 0.8701, -2.7126, -0.0924, -0.5735],\n",
      "         [-0.3022, -0.6009,  0.7209, -3.0595],\n",
      "         [-0.4824,  0.2287,  1.7308,  0.8694],\n",
      "         [ 0.6027, -0.5183,  1.1748,  0.7596],\n",
      "         [-1.7055,  1.2976,  2.6416,  0.7050]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now let's add\n",
    "\n",
    "new_embeddings = new_embeddings + residual_embeddings_1\n",
    "\n",
    "print(new_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Cross Attention ‼️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved to add after the attention step\n",
    "cross_residual = new_embeddings.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**👇🏼 Pre Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For normalization we actually have to use trainable parameters\n",
    "epsilon = nn.Parameter(torch.ones(embed_dim))\n",
    "gamma = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "input_embed = F.layer_norm(input_embed, epsilon.shape, epsilon, gamma, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1133, 0.5196, 0.1891, 0.5066],\n",
       "         [0.5957, 0.7000, 0.6619, 0.1435],\n",
       "         [0.4222, 0.3409, 0.8789, 0.5092],\n",
       "         [0.1700, 0.0887, 0.6071, 0.2530],\n",
       "         [0.9986, 0.9972, 0.7730, 0.6266],\n",
       "         [0.9470, 0.9322, 0.8488, 0.1389],\n",
       "         [0.0336, 0.3528, 0.9591, 0.1084],\n",
       "         [0.7983, 0.5002, 0.2624, 0.9730],\n",
       "         [0.0630, 0.9560, 0.6581, 0.3984],\n",
       "         [0.6781, 0.1505, 0.9560, 0.0680]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume these to be embeddings that come from the encoder\n",
    "cross_embeddings = torch.rand(1, 10, 4)\n",
    "cross_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 4]) torch.Size([1, 10, 4]) torch.Size([1, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "# This Linear layer will help us split our embeddings into Query, Key, and Value\n",
    "kv_layer = nn.Linear(embed_dim, 2 * embed_dim, bias=False)\n",
    "q_layer = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "k, v = kv_layer(cross_embeddings).split(embed_dim, dim=2)\n",
    "q = q_layer(new_embeddings)\n",
    "print(q.size(), k.size(), v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9206,  0.3820, -0.2387, -1.1162],\n",
      "         [-0.3493,  0.1047,  0.7024,  0.0202],\n",
      "         [ 1.8250,  0.1949, -0.6655,  0.1835],\n",
      "         [ 1.2321,  0.2170, -0.2109,  0.0787],\n",
      "         [ 2.1834,  0.0294, -0.4852,  1.6570],\n",
      "         [ 1.1106,  0.2156, -0.7566, -0.7907],\n",
      "         [-0.2466, -0.0755, -1.1325, -1.3882],\n",
      "         [ 0.0185, -0.2302, -0.4079,  0.5411],\n",
      "         [ 0.6773, -0.0992, -0.6065,  0.4018],\n",
      "         [-0.7860, -0.4336, -0.3595,  0.6637]]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[[ 0.9206,  0.3820],\n",
      "          [-0.3493,  0.1047],\n",
      "          [ 1.8250,  0.1949],\n",
      "          [ 1.2321,  0.2170],\n",
      "          [ 2.1834,  0.0294],\n",
      "          [ 1.1106,  0.2156],\n",
      "          [-0.2466, -0.0755],\n",
      "          [ 0.0185, -0.2302],\n",
      "          [ 0.6773, -0.0992],\n",
      "          [-0.7860, -0.4336]],\n",
      "\n",
      "         [[-0.2387, -1.1162],\n",
      "          [ 0.7024,  0.0202],\n",
      "          [-0.6655,  0.1835],\n",
      "          [-0.2109,  0.0787],\n",
      "          [-0.4852,  1.6570],\n",
      "          [-0.7566, -0.7907],\n",
      "          [-1.1325, -1.3882],\n",
      "          [-0.4079,  0.5411],\n",
      "          [-0.6065,  0.4018],\n",
      "          [-0.3595,  0.6637]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(q)\n",
    "\n",
    "k = k.view(batch_size, seq_len, n_head, embed_dim // n_head).transpose(1, 2) # (Batch size, num_heads, sequence length, embeddings per head)\n",
    "q = q.view(batch_size, seq_len, n_head, embed_dim // n_head).transpose(1, 2) # (Batch size, num_heads, sequence length, embeddings per head)\n",
    "v = v.view(batch_size, seq_len, n_head, embed_dim // n_head).transpose(1, 2) # (Batch size, num_heads, sequence length, embeddings per head)\n",
    "\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 10, 10])\n",
      "torch.Size([1, 2, 10, 10])\n",
      "tensor([[[-0.5236,  0.6212, -0.4255, -0.0544],\n",
      "         [-0.4938,  0.5861, -0.4240, -0.0541],\n",
      "         [-0.5406,  0.6432, -0.4012, -0.0636],\n",
      "         [-0.5287,  0.6280, -0.4094, -0.0604],\n",
      "         [-0.5464,  0.6511, -0.3836, -0.0690],\n",
      "         [-0.5262,  0.6248, -0.4133, -0.0594],\n",
      "         [-0.4945,  0.5872, -0.4160, -0.0588],\n",
      "         [-0.4990,  0.5928, -0.4001, -0.0638],\n",
      "         [-0.5145,  0.6110, -0.3991, -0.0642],\n",
      "         [-0.4794,  0.5706, -0.3991, -0.0640]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "attention = (q @ k.transpose(-2, -1)) / math.sqrt(embed_dim // n_head) # In multi-head attention, Dk becomes the dimension of embeddings per head\n",
    "\n",
    "print(attention.size())\n",
    "\n",
    "# Cross Attention does not need a causal mask\n",
    "\n",
    "attention = F.softmax(attention, dim=-1)\n",
    "\n",
    "print(attention.size())\n",
    "\n",
    "new_embeddings = attention @ v\n",
    "new_embeddings = new_embeddings.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim) # re-assemble all head outputs side by side\n",
    "\n",
    "print(new_embeddings)\n",
    "print(new_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4341, -3.0259, -1.2910, -0.4045],\n",
      "         [-1.6543, -0.0179, -0.8725,  0.8440],\n",
      "         [ 0.7140, -2.6464,  0.2031,  1.7223],\n",
      "         [ 1.5007, -0.4726, -1.3585,  0.3493],\n",
      "         [ 3.4838,  0.9198, -0.4342,  2.4278],\n",
      "         [ 0.3439, -2.0877, -0.5058, -0.6329],\n",
      "         [-0.7967, -0.0137,  0.3050, -3.1183],\n",
      "         [-0.9814,  0.8215,  1.3307,  0.8056],\n",
      "         [ 0.0882,  0.0927,  0.7757,  0.6954],\n",
      "         [-2.1849,  1.8682,  2.2425,  0.6410]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now let's add\n",
    "\n",
    "new_embeddings = new_embeddings + cross_residual\n",
    "\n",
    "print(new_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_embeddings_2 = new_embeddings.clone() # Again we keep track inorder to add them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For normalization we actually have to use trainable parameters\n",
    "epsilon = nn.Parameter(torch.ones(embed_dim))\n",
    "gamma = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "new_embeddings = F.layer_norm(new_embeddings, epsilon.shape, epsilon, gamma, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Feed-forward part is responsible to help the model learn more\n",
    "\n",
    "make_big = nn.Linear(embed_dim, 4 * embed_dim, bias=False)\n",
    "gelu = nn.GELU()\n",
    "make_small = nn.Linear(4 * embed_dim, embed_dim, bias=False)\n",
    "\n",
    "new_embeddings = make_big(new_embeddings)\n",
    "new_embeddings = gelu(new_embeddings)\n",
    "new_embeddings = make_small(new_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5406, -3.0757, -1.2003, -0.2461],\n",
      "         [-1.4723,  0.2819, -0.9948,  0.8720],\n",
      "         [ 0.6188, -2.6957,  0.2549,  1.8696],\n",
      "         [ 1.5030, -0.2592, -1.2548,  0.3540],\n",
      "         [ 3.5007,  1.1581, -0.3409,  2.4188],\n",
      "         [ 0.2270, -2.1522, -0.3466, -0.3734],\n",
      "         [-0.7870, -0.2191,  0.3569, -3.0667],\n",
      "         [-0.9544,  0.8948,  1.1473,  0.8998],\n",
      "         [ 0.0308,  0.1222,  0.6844,  0.9148],\n",
      "         [-2.1406,  1.8915,  2.0670,  0.6925]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_embeddings = new_embeddings + residual_embeddings_2\n",
    "\n",
    "print(new_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And we are done!! This was the decoder process 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
